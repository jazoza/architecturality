{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running SOM on Txt: Topic Detection, Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF model\n",
    "\n",
    "In order to work with text, as described by (Sharma and Dey, 2013) or (Simoes, 2014), as well as the authors of the Java SOMToolbox at TU Wien [http://www.ifs.tuwien.ac.at/dm/somtoolbox/], text needs to be represented numerically (as descrbied in ifs' [Section on Text Representation](http://www.ifs.tuwien.ac.at/~andi/somlib/textrepresentation.html)), such as a Vector Space Model (VSM)  - using a bag-of-words approach or a more sofisticated term frequency - inverse document frequency (TF-IDF) model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordPunctTokenizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDoc(doc):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = WordPunctTokenizer().tokenize(doc)\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(31046 unique tokens: ['#bim', '#bimscotland', '#cad', '#math', '/']...)\n"
     ]
    }
   ],
   "source": [
    "tweets_all = open('Twitter_MINING/SOM-text/tweets_11_2017_all.txt')\n",
    "dictionary = gensim.corpora.Dictionary(line.lower().split() for line in tweets_all)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14351 unique tokens: ['##cpl17', '#10', '#11bienaldearquitetura', '#121seaport', '#123ddesign']...)\n"
     ]
    }
   ],
   "source": [
    "tweets_clean = open('Twitter_MINING/SOM-text/tweets_11_2017_termsonly.txt') # removed links, @mentions and stopwords\n",
    "dictionary = gensim.corpora.Dictionary(line.lower().split() for line in tweets_clean)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('Twitter_MINING/SOM-text/tweets_11_2017_all.txt'):\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(1018 documents, 14350 features, 74433 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "corpus = MyCorpus()\n",
    "gensim.corpora.MmCorpus.serialize('corpus.mm', corpus) # Save corpus to disk\n",
    "corpus = gensim.corpora.MmCorpus('corpus.mm') # Load corpus\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=1018, num_nnz=74433)\n"
     ]
    }
   ],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_and_training_data(filename):\n",
    "    '''takes the input file and returns  tokenized sentences and document tags as separate lists'''\n",
    "    tags=[]\n",
    "    documents=[]\n",
    "    line_counter=1\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            #skip first line\n",
    "            if line_counter==1:\n",
    "                line_counter=line_counter+1\n",
    "                continue\n",
    "            #Initialize the token list for line\n",
    "            tags.append(line[:1])\n",
    "            documents.append(line[2:])\n",
    "    return tags,documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y,X=get_tag_and_training_data('Twitter_MINING/SOM-text/tweets_11_2017_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#75:25 training test split\n",
    "Y_train,Y_test=Y[:4120],Y[4120:]\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(X)\n",
    "freq_term_matrix = count_vectorizer.transform(X)\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)\n",
    "tf_idf_matrix = tfidf.transform(freq_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6886982311195848"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train logistic regression model\n",
    "X_train,X_test=tf_idf_matrix[:4120],tf_idf_matrix[4120:]\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train,Y_train)\n",
    "pred=logreg.predict(X_test)\n",
    "accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['R'], dtype='<U1')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd Category is BUSINESS NEWS,so lets test out a news peice on TESLA\n",
    "logreg.predict(tfidf.transform((count_vectorizer.transform([\"RT @Moneypenny: Massive congratulations to @AEWarchitects for their Corporate Workplace win at the #bcoawards. We absolutely ❤️ our… \"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5727147547929244"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,Y_train)\n",
    "nb_pred=clf.predict(X_test)\n",
    "accuracy_score(Y_test, nb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data already loaded as lists of sentences in X and Y\n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(X):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "model = doc2vec.Doc2Vec(docs, vector_size = 160, window = 10, min_count = 7, workers = 4)\n",
    "\n",
    "#making training and test sets\n",
    "wb_Y_train,wb_Y_test=Y_train,Y_test\n",
    "wb_X=[]\n",
    "for i in range(len(X)):\n",
    "    wb_X.append(model.docvecs[i])\n",
    "wb_X_train=wb_X[:4120]\n",
    "wb_X_test=wb_X[4120:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5582035801292236"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_logreg = linear_model.LogisticRegression(C=1e4)\n",
    "wb_logreg.fit(wb_X_train,wb_Y_train)\n",
    "wb_pred=wb_logreg.predict(wb_X_test)\n",
    "accuracy_score(wb_Y_test, wb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wb_X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-4a00edcd069e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwb_Y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwb_nb_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwb_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_Y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb_nb_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wb_X_train' is not defined"
     ]
    }
   ],
   "source": [
    "wb_clf = GaussianNB()\n",
    "wb_clf.fit(wb_X_train,wb_Y_train)\n",
    "wb_nb_pred=wb_clf.predict(wb_X_test)\n",
    "accuracy_score(wb_Y_test, wb_nb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopkins test for cluster tendency\n",
    "Determines whether or not a data set contains clusters. The closer the output value is to 1, the higher is the cluster tendency.\n",
    "from: https://matevzkunaver.wordpress.com/2017/06/20/hopkins-test-for-cluster-tendency/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import sample\n",
    "from numpy.random import uniform\n",
    "import numpy as np\n",
    "from math import isnan\n",
    " \n",
    "def hopkins(X):\n",
    "    d = X.shape[1]\n",
    "    #d = len(vars) # columns\n",
    "    n = len(X) # rows\n",
    "    m = int(0.1 * n) # heuristic from article [1]\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n",
    " \n",
    "    rand_X = sample(range(0, n, 1), m)\n",
    " \n",
    "    ujd = []\n",
    "    wjd = []\n",
    "    for j in range(0, m):\n",
    "        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n",
    "        ujd.append(u_dist[0][1])\n",
    "        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n",
    "        wjd.append(w_dist[0][1])\n",
    " \n",
    "    H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "    if isnan(H):\n",
    "        print ujd, wjd\n",
    "        H = 0\n",
    " \n",
    "    return H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
