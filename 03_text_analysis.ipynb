{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 | Tweets analysis\n",
    "\n",
    "\n",
    "## Prepare the Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions (Twitter specific)\n",
    "\n",
    "Separate in 'tokens', translate special characters and special expressions (emoticons), remove stopwords. Below are some basic preprocessing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, re, operator, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['RT', 'The', 'rt', 'via', 'amp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the tweets > SCRAPING\n",
    "\n",
    "Downloading all tweets from a list of users; storing userID, text, timestamp\n",
    "\n",
    "The first analysis on the text is made using the tweet text, which has been imported from the eight .csv files collected with the scraping script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# load .csv files created by SCRAPING\n",
    "files = [f for f in listdir('./') if f.endswith('.csv') and isfile(join('./', f))]\n",
    "d_scrape = pd.concat([pd.read_csv(f) for f in files], keys=files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peprocess the tweet text and create a string of tweet words to analyse for word frequency and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the text column from the dataframe to a string\n",
    "tweets_l1 = d_scrape['text'].tolist() # create a list from 'text' column in d dataframe\n",
    "scrape_terms_only = [term for term in preprocess(' '.join(tweets_l1)) if term not in stop and not term.startswith(('@', 'http'))] # remove mentions and links - useful only for wordcloud approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the tweets > STREAMING\n",
    "\n",
    "Prograsivelly stream all tweets from a list of users; gathers all available data and metadata into a .json file\n",
    "\n",
    "The second, or parallel analysis is made on the text of tweets which are imported from a collection of tweets from profiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load .json files created by STREAMING\n",
    "files = [f for f in listdir('/path/to/files') if f.endswith('.json') and isfile(join('/path/to/files', f))]\n",
    "d_stream = pd.concat([pd.read_json(f, lines=True) for f in files], keys=files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peprocess the tweet text and create a string of tweet words to analyse for word frequency and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the text column from the dataframe to a string\n",
    "tweets_l2 = d_stream['text'].tolist() # create a list from 'text' column in d dataframe\n",
    "stream_terms_only = [term for term in preprocess(' '.join(tweets_l2)) if term not in stop and not term.startswith(('@', 'http'))] # remove mentions and links - useful only for wordcloud approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud\n",
    "\n",
    "Create a wordcloud from the list of tweets created with the above process. Using the [wordcloud library](https://github.com/amueller/word_cloud) by [Andreas Mueller](https://github.com/amueller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# specify fonts, stopwords, background color and other options\n",
    "wordcloud = WordCloud(font_path='/Users/zoza/Library/Fonts/CooperHewitt-Bold.otf',\n",
    "                          stopwords=open('Twitter_MINING/twitter-sentiment-analysis-stopwords.txt').read().split(),\n",
    "                          background_color='white',\n",
    "                          width=2400,\n",
    "                          height=2000\n",
    "                         ).generate(tweets)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates this image:\n",
    "![Wordcloud of tweets collected from the list of eight profiles, all (from 27. 02. to 28. 02. 2017)](https://goo.gl/photos/8xnJc2jhhhgezR3j7)\n",
    "\n",
    "### Keywords extraction\n",
    "\n",
    "Extracting keywords from the list of tweets created with the above process.\n",
    "\n",
    "First, tested a [Python implementation of the Rapid Automatic Keyword Extraction (RAKE) algorithm] (https://github.com/zelandiya/RAKE-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rake, operator\n",
    "rake_object = rake.Rake(\"SmartStoplist.txt\", 4, 2, 4) # words of minimum length 4, in groups of maximum 2, occurring at least 3 times in the text; this happens to give the best results with the particular corpus\n",
    "\n",
    "keywords = rake_object.run(tweets)\n",
    "#write the list of tuples to a file:\n",
    "outfile = open('keyword_extractedkwords.txt', 'w')\n",
    "for item in keywords:\n",
    "  keyword = item[0]\n",
    "  relevance = item[1]\n",
    "  try:\n",
    "    outfile.write(str(keyword.decode('utf-8'))+' '+str(relevance)+'\\n')\n",
    "  except UnicodeEncodeError:\n",
    "    outfile.write(str(keyword)+' '+str(relevance)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not that convincing and do not reflect the word frequency represented before. Also, they depend a lot on the parameters with which Rake is called (word length, word group size, occurrence). Below are the best results:\n",
    "\n",
    "`strategy motivation 4.0\n",
    "followers usa 4.0\n",
    "followers spain 4.0\n",
    "free map 3.875\n",
    "wanna call 3.77380952381\n",
    "posted photo 3.76470588235\n",
    "blog post 3.76050591595\n",
    "find files 3.44957983193\n",
    "files find 3.44957983193\n",
    "daily build 3.41025641026\n",
    "architecture license 3.32941176471\n",
    "creative work 3.26612903226\n",
    "architect podcast 3.15503875969\n",
    "post 1.80701754386\n",
    "architecture 1.8\n",
    "nodes 1.77777777778\n",
    "data 1.76923076923\n",
    "posted 1.76470588235\n",
    "show 1.75\n",
    "design 1.75\n",
    "find 1.73529411765\n",
    "building 1.72413793103\n",
    "read 1.72\n",
    "part 1.71428571429\n",
    "list 1.70588235294\n",
    "projects 1.69230769231\n",
    "sounds 1.69230769231\n",
    "space 1.6875\n",
    "hear 1.6875\n",
    "future 1.6875\n",
    "class 1.6875\n",
    "today 1.68518518519\n",
    "node 1.68421052632\n",
    "join 1.68421052632\n",
    "tools 1.67741935484\n",
    "check 1.67441860465\n",
    "code 1.66666666667\n",
    "podcast 1.66666666667\n",
    "revit 1.66666666667\n",
    "twitter 1.66666666667\n",
    "great 1.66666666667\n",
    "live 1.66666666667\n",
    "tool 1.66666666667\n",
    "coming 1.65517241379\n",
    "happy 1.65\n",
    "people 1.64864864865\n",
    "humans 1.63636363636\n",
    "missed 1.63636363636\n",
    "printed 1.63636363636\n",
    "make 1.62962962963\n",
    "workshop 1.625\n",
    "dynamo 1.6170212766\n",
    "watch 1.61538461538\n",
    "thinking 1.61111111111\n",
    "start 1.60714285714\n",
    "back 1.60714285714\n",
    "send 1.6\n",
    "`\n",
    "\n",
    "Another small script: [keyword_extraction_w_parser](https://github.com/naushadzaman/keyword-extraction-from-tweets) (made for twitter specifically) provides a very flexible way to extract keywords, and works relatively well with the entire collection of tweets converted into a single string (as explained in: Preprocessing the text of tweets). For this purpose, I modified the string to include #hashtags and @mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = '' # tweets are an empty string\n",
    "for item in tweets_l:\n",
    "  terms_only = [term for term in preprocess(unicode(item, errors='ignore')) if term not in stop]\n",
    "  for terms in terms_only:\n",
    "    tweets = tweets + ' ' + terms\n",
    "\n",
    "import keyword_extraction_w_parser\n",
    "keywords2=keyword_extraction_w_parser.get_keywords(tweets)\n",
    "outfile = open('keywords_tweets_extractedkwords.txt', 'w')\n",
    "for item in keywords2:\n",
    "  try:\n",
    "    outfile.write(str(item.decode('utf-8'))+'\\n')\n",
    "  except UnicodeEncodeError:\n",
    "    outfile.write(str(item)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 56 results (same number as above):\n",
    "\n",
    "`get;\n",
    "revit viewport outline;\n",
    "dynamobim;\n",
    "kcmarchitect nice houmanave;\n",
    "package node library revitaddons;\n",
    "dynamo player webinar video;\n",
    "evolve_lab ht;\n",
    "kcmarchitect;\n",
    "nodes;\n",
    "houmanave;\n",
    "zero touch;\n",
    "kcmarchitect houmanave dlls bin folder houmanave new version;\n",
    "celery;\n",
    "end march major bug fixes brand;\n",
    "new node;\n",
    "dynamobim houmanave;\n",
    "everyone;\n",
    "pre-beta;\n",
    "celery dynamobim;\n",
    "try;\n",
    "large geometries;\n",
    "dynamo4revit worry;\n",
    "release 1.3 dynamobim;\n",
    "alpha release;\n",
    "butterfly available dynamobim;\n",
    "graph mapper ftw;\n",
    "dynamobim;\n",
    "emojipedia;\n",
    "one;\n",
    "flux_io love;\n",
    "slackhq;\n",
    "check labs integration;\n",
    "bim grasshopper script channel;\n",
    "hug;\n",
    "awesome radugidei;\n",
    "made prototype tonight;\n",
    "user;\n",
    "document close;\n",
    "cool space truss man;\n",
    "converts revit models uninstalls;\n",
    "good;\n",
    "dynamobim graph;\n",
    "cad plans;\n",
    "revit models;\n",
    "thebimsider;\n",
    "doe folks;\n",
    "john kind;\n",
    "enough share doe dataset thebimsider thanks;\n",
    "doc event;\n",
    "last night john;\n",
    "kind;\n",
    "enough share dataset dynamobim;\n",
    "right;\n",
    "yet user experience;\n",
    "message dynamobim;\n",
    "beta tester dynamobim evolve_lab;\n",
    "lablive free webinar topic;\n",
    "`\n",
    "\n",
    "However, this list is highly imperfect. A probably more efficient approach is to rely on #hashtags as keywords users themselves have identified.\n",
    "\n",
    "#### Hashtags\n",
    "\n",
    "The list of unique hashtags was obtained from the same collection of tweets used above. In a total of 18473 tweets, there are 12785 hashtags, less than a quarter of them unique - 3607 words. These words are stored in a file (which file)\n",
    "\n",
    "\n",
    "## STREAMING\n",
    "Collecting tweets as they come, using the tweepy streaming method. Storing all available data on tweets in a .json file.\n",
    "\n",
    "### Loading the tweets\n",
    "The process of loading the tweets is the same, with the difference in file extension (.json instead of .csv) and thus the reading call (read_json instead of read_csv). Loaded tweets however, contain much more information. For example, it is possible to extract the attached media (see bellow: [Extracting images (media)](extractingimages)) or geolocation from the collected tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load .json files created by STREAMING\n",
    "files = [f for f in listdir('/path/to/files') if f.endswith('.json') and isfile(join('/path/to/files', f))]\n",
    "d = pd.concat([pd.read_json(f, lines=True) for f in files], keys=files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, using the same preprocessing functions and method for selecting only tweet text from the dataframe, we prepare a string of tweets.\n",
    "\n",
    "As discussed in 02_gathering-tweets, two sets of tweets were collected using the STREAMING method: tweets filtered by keyword and tweets filtered by profile. This is an opportunity to compare these collections and their relevance to the reserach.\n",
    "-rlt\n",
    "#### PROFILE\n",
    "tweets (number of words): 227166\n",
    "tweets_l (number of tweets): 3632\n",
    "hashtags: 511 unique (total of 2326)\n",
    "\n",
    "## Further analysis (images etc.)\n",
    "\n",
    "### <a name=\"extractingimages\"></a>Extracting images (media) from collected tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_json('profile_tweets.json', lines=True)\n",
    "\n",
    "for label, value in df_profile.entities.iteritems(): # iterate through the dictionary of entities\n",
    "  if type(value)==dict and 'media' in value.keys(): # some value.keys are nan, and their type is 'float'\n",
    "    for thing in value['media']:\n",
    "      urllib.urlretrieve(thing['media_url'],'%s.jpg' %label) # save all images with the unique of their index in the df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###EXTRAS\n",
    "\n",
    "Check json file for errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse():\n",
    "  files = [f for f in listdir('.') if f.endswith('.json') and isfile(join('.', f))]\n",
    "  for afile in files:\n",
    "    with open(afile) as json_file:\n",
    "      try:\n",
    "        json.load(json_file)\n",
    "        read_json_files.append(afile)\n",
    "      except ValueError, e:\n",
    "        print (\"JSON object issue: %s\") % e\n",
    "        invalid_json_files.append(afile)\n",
    "  print invalid_json_files, len(read_json_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
