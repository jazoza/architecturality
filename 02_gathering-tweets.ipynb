{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 |Â Gathering Tweets\n",
    "Streaming tweets by 1)profile and 2)keywords.\n",
    "\n",
    "Gathering the tweets is done using the [tweepy](https://github.com/tweepy/tweepy) module for authentication with the API and scraping or streaming of tweets.\n",
    "\n",
    "## Authentication\n",
    "\n",
    "Authentication is done using this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "# User credentials to access Twitter API\n",
    "access_token = \"ACCESS TOKEN\"\n",
    "access_token_secret = \"ACCESS TOKEN SECRET\"\n",
    "consumer_key = \"CONSUMER KEY\"\n",
    "consumer_secret = \"CONSUMER SECRET\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two gathering methods (by keyword and by profiles) are authenticating to two different apps - which had to be created so that I could simultaneously make the calls (interfering calls using the same auth would return 420 error on one streamer).\n",
    "\n",
    "Scripts in this repository use an auth.py file where the credentials for all Twitter apps are stored. A model of this script is included in the repository, under the name `auth_model.py`.\n",
    "\n",
    "## Scraping\n",
    "\n",
    "Getting all the tweets from a particular user, or a list of users.\n",
    "The [GET statuses/user timeline method](https://dev.twitter.com/rest/reference/get/statuses/user_timeline) is limited to the latest 3200 tweets by the Twitter API.\n",
    "The script gets 200 initial tweets and then extends the list of `alltweets` to include the maximum number. Tweets are then stored in a .csv file, where *userID*, *tweet text* and *time of the tweet* are written line by line. The script stores separate files for each user.\n",
    "\n",
    "## Streaming\n",
    "\n",
    "Streaming is done by listening for specific **keywords** and to specific **profiles**, selected previously. The streamer class is created, and it opens a .json file to write the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        #print data\n",
    "        with open('output.json','a') as tf:\n",
    "            tf.write(data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The streamer is then running in a loop, listening to either keywords or profiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # this is to that the script can be interrupted (ctrl+c)\n",
    "    while True:\n",
    "        if __name__ == '__main__':\n",
    "            # Instantiate the StdOutListener\n",
    "            l = StdOutListener()\n",
    "            stream = Stream(auth, l)\n",
    "            # Filter tweets by keyword\n",
    "            alltweets=stream.filter(track=keywords)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between the two streamers (apart from the app credentials) is the last line of code:\n",
    "\n",
    "`alltweets=stream.filter(track=keywords)` for keyword streaming and  `stream.filter(follow=userID_list)` for profile streaming\n",
    "\n",
    "Profile streaming requires the list of profiles to be converted from screen names to userIDs. This is done using the following script\n",
    "\n",
    "**twythonize.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twython import Twython\n",
    "from auth import TwitterAuthProfiles\n",
    "import pickle\n",
    "\n",
    "#Twython AUTHENTICATE (Oauth2)\n",
    "#Obtain an OAuth 2 Access Token\n",
    "twitter = Twython(TwitterAuthProfiles.consumer_key, TwitterAuthProfiles.consumer_secret, oauth_version=2)\n",
    "ACCESS_TOKEN = twitter.obtain_access_token()\n",
    "#Use the Access Token\n",
    "twitter = Twython(TwitterAuthProfiles.consumer_key, access_token=ACCESS_TOKEN)\n",
    "\n",
    "BIM_list=['profiles_appearing_in_search_for_BIM']\n",
    "DynamoBIM_list=['profiles_appearing_in_search_for_DynamoBIM']\n",
    "Revit_list=['profiles_appearing_in_search_for_revit']\n",
    "DigArch_list=['profiles_appearing_in_search_for_digital_architecture']\n",
    "Parametric_list=['profiles_appearing_in_search_for_parametric']\n",
    "other_list=['other_relevant_profiles']\n",
    "# adding a list of profiles who were tweeting the keywords from the second stream\n",
    "keywords_list = []\n",
    "\n",
    "#list of all lists\n",
    "screen_names_list=list(set(BIM_list+DynamoBIM_list+Revit_list+DigArch_list+Parametric_list+other_list+keywords_list))\n",
    "#Convert the list of Twitter screen names into a coma separated string, removing @signs into a string\n",
    "screen_names_withoutat = []\n",
    "for i in screen_names_list:\n",
    "  screen_names_withoutat.append(i[1:])\n",
    "\n",
    "# Query twitter with the comma separated list\n",
    "userID_list = []\n",
    "for user in screen_names_withoutat:\n",
    "    userID=twitter.lookup_user(screen_name=user)\n",
    "    userID_list.append(str(userID[0][\"id_str\"]))\n",
    "\n",
    "# write the list of usedIDs to a file\n",
    "with open('userID.txt', 'a') as f:\n",
    "    pickle.dump(userID_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each collection of tweets is stored in a .json file on the server, constantly updated. Tweets were collected: from `Fri, 31 Mar 2017 08:51:20.365 GMT` to `Wed, 12 Apr 2017 03:19:32.214 GMT` (profile_keywords.json, 20.5MB, 3632 tweets), from `Wed, 05 Apr 2017 12:58:49.914 GMT` to `Wed, 12 Apr 2017 02:55:44.893 GMT` (keyword_tweets.json, 126.4MB, 26994 tweets) and from `Thu, 30 Mar 2017 14:58:07.471 GMT` to `Fri, 31 Mar 2017 09:50:14.120 GMT` (keyword_SPOILED_tweets.json, 55.5MB). The *keyword* collection file is growing more quickly, but the *profile* collection seems to have more meaningful content."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
